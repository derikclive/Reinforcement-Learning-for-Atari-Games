{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "[2018-01-06 19:42:19,536] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization,Convolution2D,Conv2D, LeakyReLU,ELU,Input, UpSampling2D, Activation, merge, MaxPooling2D, Deconvolution2D, Reshape, Permute\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "from statistics import median, mean\n",
    "from collections import Counter\n",
    "\n",
    "LR = 1e-3\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "goal_steps = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The idea of CartPole is that there is a pole standing up on top of a cart. The goal is to balance this pole by wiggling/moving the cart from side to side to keep the pole balanced upright.\n",
    "\n",
    "The environment is deemed successful if we can balance for 200 frames, and failure is deemed when the pole is more than 15 degrees from fully vertical.\n",
    "\n",
    "Every frame that we go with the pole \"balanced\" (less than 15 degrees from vertical), our \"score\" gets +1, and our target is a score of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def some_random_games_first():\n",
    "    # Each of these is its own game.\n",
    "    for episode in range(5):\n",
    "        env.reset()\n",
    "        # this is each frame, up to 200...but we wont make it that far.\n",
    "        for t in range(200):\n",
    "            # This will display the environment\n",
    "            # Only display if you really want to see it.\n",
    "            # Takes much longer to display it.\n",
    "            #env.render()\n",
    "            \n",
    "            # This will just create a sample action in any environment.\n",
    "            # In this environment, the action can be 0 or 1, which is left or right\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # this executes the environment with an action, \n",
    "            # and returns the observation of the environment, \n",
    "            # the reward, if the env is over, and other info.\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "some_random_games_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training Data\n",
    "Here we create the training data by playing the games using random actions(left/right) and storing the observations and the actions associated with each oberservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_population(model=False, num_games= 5000, score_requirement = 50):\n",
    "    # [OBS, MOVES]\n",
    "    training_data = []\n",
    "    # all scores:\n",
    "    scores = []\n",
    "    # just the scores that met our threshold:\n",
    "    accepted_scores = []\n",
    "    # iterate through however many games we want:\n",
    "    for _ in range(num_games):\n",
    "        score = 0\n",
    "        # moves specifically from this environment:\n",
    "        game_memory = []\n",
    "        # previous observation that we saw\n",
    "        prev_observation = []\n",
    "        # for each frame in 200\n",
    "        for _ in range(goal_steps):\n",
    "            # choose random action (0 or 1)\n",
    "            if not model or len(prev_observation)==0:\n",
    "                action = random.randrange(0,2)\n",
    "            else:\n",
    "                action = np.argmax(model.predict(prev_observation.reshape(-1,len(prev_observation)))[0])\n",
    "            # do it!\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            \n",
    "            # notice that the observation is returned FROM the action\n",
    "            # so we'll store the previous observation here, pairing\n",
    "            # the prev observation to the action we'll take.\n",
    "            if len(prev_observation) > 0 :\n",
    "                game_memory.append([prev_observation, action])\n",
    "            prev_observation = observation\n",
    "            score+=reward\n",
    "            if done: break\n",
    "\n",
    "        # IF our score is higher than our threshold, we'd like to save\n",
    "        # every move we made\n",
    "        # NOTE the reinforcement methodology here. \n",
    "        # all we're doing is reinforcing the score, we're not trying \n",
    "        # to influence the machine in any way as to HOW that score is \n",
    "        # reached.\n",
    "        \n",
    "        if score >= score_requirement:\n",
    "            #print(score)\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                # convert to one-hot (this is the output layer for our neural network)\n",
    "                if data[1] == 1:\n",
    "                    output = [0,1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1,0]\n",
    "                    \n",
    "                # saving our training data\n",
    "                training_data.append([data[0], output])\n",
    "\n",
    "        # reset env to play again\n",
    "        env.reset()\n",
    "        # save overall scores\n",
    "        scores.append(score)\n",
    "    \n",
    "    # just in case you wanted to reference later\n",
    "    training_data_save = np.array(training_data)\n",
    "    np.save('saved.npy',training_data_save)\n",
    "    \n",
    "    # some stats here, to further illustrate the neural network magic!\n",
    "    print('Average accepted score:',mean(accepted_scores))\n",
    "    print('Median score for accepted scores:',median(accepted_scores))\n",
    "    print(Counter(accepted_scores))\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network\n",
    "\n",
    "Now we will make our neural network. We're just going to use a simple multilayer perceptron model. We use the keras API to build our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(input_size):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', input_dim=input_size))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR),metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(training_data, model=False):\n",
    "\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]))\n",
    "    y = [i[1] for i in training_data]\n",
    "\n",
    "    print(X.shape)\n",
    "    if not model:\n",
    "        model = network(input_size = len(X[0]))\n",
    "    \n",
    "    model.fit(X, y, epochs=3, verbose=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-06 19:48:24,228] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 60.65384615384615\n",
      "Median score for accepted scores: 57.0\n",
      "Counter({51.0: 16, 50.0: 16, 52.0: 16, 53.0: 15, 54.0: 13, 58.0: 10, 57.0: 9, 59.0: 8, 72.0: 6, 56.0: 6, 55.0: 6, 64.0: 5, 65.0: 5, 67.0: 5, 66.0: 5, 75.0: 4, 78.0: 4, 60.0: 3, 77.0: 3, 63.0: 3, 62.0: 3, 68.0: 2, 74.0: 2, 69.0: 2, 61.0: 2, 90.0: 2, 93.0: 2, 83.0: 1, 70.0: 1, 73.0: 1, 80.0: 1, 84.0: 1, 148.0: 1, 92.0: 1, 107.0: 1, 76.0: 1})\n",
      "(10857, 4)\n",
      "Epoch 1/5\n",
      "10857/10857 [==============================] - 3s 252us/step - loss: 0.6890 - acc: 0.5571\n",
      "Epoch 2/5\n",
      "10857/10857 [==============================] - 2s 186us/step - loss: 0.6803 - acc: 0.5863\n",
      "Epoch 3/5\n",
      "10857/10857 [==============================] - 2s 175us/step - loss: 0.6770 - acc: 0.5899\n",
      "Epoch 4/5\n",
      "10857/10857 [==============================] - 2s 178us/step - loss: 0.6751 - acc: 0.5985\n",
      "Epoch 5/5\n",
      "10857/10857 [==============================] - 2s 177us/step - loss: 0.6733 - acc: 0.5933\n"
     ]
    }
   ],
   "source": [
    "training_data = initial_population(False)\n",
    "model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the aldready trained model to predict what action to take next given an observation, instead of random actions. We then use these new obervations to further train our model to improve the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 200.0\n",
      "Median score for accepted scores: 200.0\n",
      "Counter({200.0: 60})\n",
      "(11940, 4)\n",
      "Epoch 1/5\n",
      "11940/11940 [==============================] - 2s 163us/step - loss: 0.3438 - acc: 0.8297\n",
      "Epoch 2/5\n",
      "11940/11940 [==============================] - 2s 160us/step - loss: 0.2585 - acc: 0.8752\n",
      "Epoch 3/5\n",
      "11940/11940 [==============================] - 2s 177us/step - loss: 0.1752 - acc: 0.9285\n",
      "Epoch 4/5\n",
      "11940/11940 [==============================] - 2s 190us/step - loss: 0.1642 - acc: 0.9365\n",
      "Epoch 5/5\n",
      "11940/11940 [==============================] - 2s 206us/step - loss: 0.1494 - acc: 0.9405\n",
      "Average accepted score: 200.0\n",
      "Median score for accepted scores: 200.0\n",
      "Counter({200.0: 74})\n",
      "(14726, 4)\n",
      "Epoch 1/5\n",
      "14726/14726 [==============================] - 3s 178us/step - loss: 0.1264 - acc: 0.9516\n",
      "Epoch 2/5\n",
      "14726/14726 [==============================] - 3s 184us/step - loss: 0.1062 - acc: 0.9584\n",
      "Epoch 3/5\n",
      "14726/14726 [==============================] - 3s 194us/step - loss: 0.1072 - acc: 0.9552\n",
      "Epoch 4/5\n",
      "14726/14726 [==============================] - 3s 196us/step - loss: 0.0996 - acc: 0.9613\n",
      "Epoch 5/5\n",
      "14726/14726 [==============================] - 3s 188us/step - loss: 0.0961 - acc: 0.9604\n",
      "Average accepted score: 199.66666666666666\n",
      "Median score for accepted scores: 200.0\n",
      "Counter({200.0: 84, 192.0: 2, 187.0: 1})\n",
      "(17284, 4)\n",
      "Epoch 1/5\n",
      "17284/17284 [==============================] - 3s 188us/step - loss: 0.0900 - acc: 0.9634\n",
      "Epoch 2/5\n",
      "17284/17284 [==============================] - 3s 192us/step - loss: 0.0903 - acc: 0.9634\n",
      "Epoch 3/5\n",
      "17284/17284 [==============================] - 3s 192us/step - loss: 0.0950 - acc: 0.9606\n",
      "Epoch 4/5\n",
      "17284/17284 [==============================] - 3s 197us/step - loss: 0.0922 - acc: 0.9627\n",
      "Epoch 5/5\n",
      "17284/17284 [==============================] - 3s 196us/step - loss: 0.0904 - acc: 0.9647\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    training_data = initial_population(model, 100, 185)\n",
    "    model = train_model(training_data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 198.4\n",
      "choice 1:0.5020161290322581  choice 0:0.49798387096774194\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(10):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    env.reset()\n",
    "    for _ in range(goal_steps):\n",
    "        #env.render()\n",
    "\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs)))[0])\n",
    "\n",
    "        choices.append(action)\n",
    "                \n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        score+=reward\n",
    "        if done: break\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
